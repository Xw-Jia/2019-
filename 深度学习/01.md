## Github: [Tensorflow深度学习](https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow)
# 第一章  深度学习入门
## 1.1 机器学习简介
> 1. 监督学习：定义损失函数，训练模型调整参数使损失函数最小化，**可以用在分类和回归问题中**，分类是输出类别，++回归是输出对应的数值++，最重要的指标是**泛化能力**
> 2. 无监督学习：处理**聚类问题**，一般会假设输入为相互独立的样本，并且服从某种概率分布
> 3. 强化学习：强调通过系统和环境的**交互**来学习，**通过环境的反馈动态调整系统的参数**
### 总结：
|监督学习|无监督学习|强化学习|
|-|-|-|
|分类问题，|聚类|决策系统| 
|回归问题，|关联规则挖掘|回报系统|
|排序问题|分割，维度约减|推荐系统|

## 1.2 深度学习定义
> **深度学习：** 建立一个多层的学习模型，**通过将浅层的输出作为深层的输入**，使数据层层转化，越来越抽象。
## 1.3 人工神经网络
> 过程很简单，多个输入，按照权重加权求和，然后激活函数，得到输出，输出传到下一个连接的神经元，再按权重参数加权求和....
## 1.4 人工神经网络的学习方式
### 1.4.1 反向传播
**步骤：**
>1. 随机权重初始化网络
>2. 对每个训练样本，重复：
	1. 前向传播：计算网络产生的总误差（由损失函数表征）
	2. 反向传播：从输出到输出，反向遍历所有层
>3. **反向传播时，根据上一层的误差和对应的权重，逐层计算内部各层误差，从而将总误差从输出到输入反向传播**
>4. 根据各层的误差，调整各层的权重，最小化损失函数，这样不断优化权重参数。
**当验证集上面的误差增加时，训练终止，因为此时可能产生了过拟合。**
### 1.4.2 权重优化
1. 梯度下降法（GD）：
	1. 随机初始化权重
	2. 对模型的每个参数，计算损失函数的梯度
	3. 负梯度方向调整模型参数
	4. 重复2,3，直到梯度G趋近为零
> GD是完整的在训练集上面计算损失函数的梯度，也称为BGD，如果数据集很大，每走一步需要遍历整个数据集，会产生特别大的开销。
**采用SGD，在一次迭代中只采用一个训练样例升级一个参数，随机的意思是，单一训练样例的梯度是真正损失函数梯度的随机近似。**
SGD受到随机的影响，收敛到全局最小代价的路径不是直的
2. 随机梯度下降法（SGD）：
> 每处理一个样本，权重就得到一次更新，后续的计算使用的是被前面优化过的权重。

## 1.5神经网络架构
> 1.多层感知机：
> 2.DNN:
> 3.

