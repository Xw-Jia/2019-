# 03TF构建前馈神经网络
> 前馈神经网络：包括一个输入层，一个或者多个隐藏层，一个输出层，每个神经元都和前一层的所有神经元相连接，连接有不同的权重。
隐藏层太多可能导致过拟合，隐藏层太少就会导致拟合精度不够

每个神经元接受到的输入信号是，与之相连的神经元的激励值的加权和。
激活函数：sigmoid函数，输出为值域（0,1）
sigmoid:
$$out_i=1/(1+e^{-net_i})$$
softmax:
$$out_{i}=\frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}}$$
ReLU: 
$$f(x)=max(0,x)$$
> 优势：函数运算速度更快，因为不需要进行指数运算，另外，可以大大加速SGD的收敛速度

Dropout：在前馈和反向传播中，未激活的神经元不会对特定输入产生影响。这样，对于每个输入，网络的结构有微小的差异。有些激活，有些放弃。这样，训练时可以对拥有大量不同神经元的网络进行平均。从而避免过拟合，很少数据量。无需改变整个网络的结构。
