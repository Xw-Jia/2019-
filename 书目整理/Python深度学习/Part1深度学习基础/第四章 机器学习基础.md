# 第四章 机器学习基础
> 除分类和回归之外的机器学习形式有什么？
怎么评估机器学习模型？
数据的准备
特征工程
过拟合怎么解决？
机器学习的通用工作流程？

### 模型评估、数据预处理、特征工程、解决过拟合====> 七步走~

## 一、ML的四个分支
>1. 监督学习：
>>分类/回归/序列生成、语法树预测、目标检测、图像分割

>2. 无监督学习
>>目的在于数据可视化和数据压缩去噪。可以在监督学习之前使用无监督，更好的理解数据。
**降维和聚类**

>3. 自监督学习
>>++**没有人工标签的监督学习，标签仍然存在，不是人工添加，但是是由输入数据，通过启发式算法生成**++。
包括：自编码器/输入视频的一帧或者单词预测下一个（时序监督学习，用未来的输入数据作为监督）

>4. 强化学习

## 二、评估ML的model
关于测试集训练会导致信息泄露，同样会导致模型的泛化能力不够
>当可用数据很少时，有三种经典的评估方法：
>1. 简单的留出验证
>2. K折验证
>3. 带打乱数据的重复K折验证
### 1. 简单的留出验证
```python
'''
简单留出验证的代码：通常需要打乱顺序
'''
num_validation_samples =  10000
#打乱顺序
np.random.shuffle(data)
validation_data = data[:num_validation_samples]	#定义验证集
data = data[num_validation_samples:]

training_data = data[:]	#定义训练集

#训练集训练，验证集评估
model = get_model()
model.train(training_data)
validation_score = model.evaluate(validation_data)

#调节模型，重新训练、评估，然后再次调节
#调节好超参数之后，要在所有的非测试集上重新训练，得到最终的模型
model = get_model()
model.train(np.concatenate([training_data, validation_data]))
test_score = model.evaluate(test_data)
```
>这个方法的问题是：如果数据很少，验证集和测试集的样本太少，所以不同的随机打乱，最终模型的性能差别就很大



