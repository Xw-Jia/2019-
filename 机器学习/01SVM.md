# 01 SVM原理及要点
**参考博客地址[微笑sun](https://www.cnblogs.com/jiangxinyang/p/9217424.html)**
>SVM原来处理二分类问题，经过进化，也可以处理多元线性分类和非线性的问题，同时处理回归问题。在深度学习兴起之前，是最好的分类算法。
**现在，在小样本的问题上，SVM仍然很好用**
## 0 函数间隔和几何间隔
对于给定的训练数据集T和超平面(w,b)，定义超平面关于样本点(x_i,y_i)的函数间隔是：
![title](https://i.loli.net/2019/03/28/5c9c5cdc88a58.png)
函数间隔可以表示分类预测的正确性及确信度，但选择分离超平面时，只有函数间隔还不够，因为只要成比例改变w和b，超平面并没有改变，但函数间隔却变了，因此需要对分离超平面的法向量加上某些约束，如规范化，||w||=1，使用间隔是确定的，这时函数间隔成为了几何间隔。
**几何间隔就是函数间隔除以w的L2范数，这样即使w,b成比例变化，几何间隔也不会改变**
![title](https://i.loli.net/2019/03/28/5c9c5d864c1d0.png)



## 1 感知机模型
>感知机模型是**二分类的线性分类器**，尝试找到超平面分开数据集，二维空间中，超平面是个直线，三维空间中则是一个平面。

感知机的模型：

$$f(x) = sign(W*x+b)$$

$sign(x)$是指示函数，自变量大于0取1，小于零取-1
![感知机](https://i.loli.net/2019/03/28/5c9c4c841e95f.png)

对于正确分类的点，$y(Wx+b) > 0$,对于错误分类的点，$<0$，所以，采用损失函数，使所有误分类的点到超平面的距离之和最小。

$$-\frac{1}{\begin{Vmatrix}w\end{Vmatrix}}\sum_{x_i\in{M}}y_i(w*x_i+b)$$

其中，M是表示**误分类的样本集合**，所以，最终得到的**感知机的损失函数**是：
$$L(w,b)=-\sum_{x_i\in{M}}y_i(w*x_i+b)$$

## 2 SVM
>感知机中，目标是将训练集分开，**而能够将样本分开的超平面有很多**。SVM本质上类似于感知机，但是要求却更加苛刻。

***远离超平面的点是安全的，容易被误分类的点离超平面很近。SVM的思想是重点关注离超平面很近的点***====>**++在分类正确的同时，让离超平面最近的点到超平面的距离最大++**
![title](https://i.loli.net/2019/03/28/5c9c52138cf6e.png)

**$\gamma$是离超平面最近的点的到超平面的几何间隔**，将几何间隔用函数间隔替代，可以将式子表示为：
![title](https://i.loli.net/2019/03/28/5c9c52a1c4577.png)
$\gamma$(^) 表示的是函数间隔，而函数间隔的取值是会随着w，b 成倍数变化而变化的，并不会影响最终的结果，因此令γ(帽子) = 1，则我们最终的问题可以表述为：
![title](https://i.loli.net/2019/03/28/5c9c5385cf832.png)
>这里得到了SVM的第一个**亮点：最大化间隔，最大化间隔能使得分类更加精确，且该最大间隔超平面是存在且唯一的。**
![title](https://i.loli.net/2019/03/28/5c9c53ff0a812.png)

根据凸优化理论，我们可以借助拉格朗日函数将我们的约束问题转化为无约束的问题来求解，我们的优化函数可以表达为：
![title](https://i.loli.net/2019/03/28/5c9c549d2d586.png)
其中，$\alpha_i$是拉格朗日乘子，都为正数。
然后，根据拉格朗日的对偶性，再转化为它的对偶问题的极大极小问题，（更容易求解）：
![title](https://i.loli.net/2019/03/28/5c9c5602d36ba.png)
先对w,b求导极小，得到w,b的值。然后，将得到的解带入拉格朗日函数，得到：
、、、、、
（省略）
、、、、、
转化成求$\alpha$的极小值问题，再引入KTT条件，得到：
yi(w*xi + b*) - 1 > 0 时，αi* = 0；当 αi* > 0 时，yi(w*xi + b*) - 1 = 0；
**得到SVM的第二个亮点：w，b 参数只与满足 yi(w*xi + b*) - 1 = 0 的样本有关，而这些样本点就是离最大间隔超平面最近的点，我们将这些点称之为*支持向量*。因此很多时候支持向量在小样本集分类时也能表现的很好，也正是因为这个原因。（另外需注意：α 向量的个数是和训练集数量相等的，对与大的训练集，会导致所需要的参数数量增多，因此SVM在处理大的训练集时会比其他常见的机器学习算法要慢）**

***因为参数只与支持向量有关，所以在小样本集上，效果很好。但是拉格朗日乘子的个数与训练集数量相等，所以SVM比照其他机器学习方法，略慢。***

## 3 软间隔最大化
>上面的硬间隔最大化无法处理线性不可分的问题，当存在异常点就会导致线性不可分。线性不可分以为之有些样本点的函数间隔不满足大于等于1的约束。因此对每个样本（xi,yi）引入一个松弛变量ξi， 则我们的约束条件变为：
![title](https://i.loli.net/2019/03/28/5c9c58ad6d377.png)

目标函数中加入对松弛变量的惩罚项，惩罚参数C > 0，目标优化函数变为：
![title](https://i.loli.net/2019/03/28/5c9c59b165bb2.png)

采用和之前同样的求解方法，利用拉格朗日将约束问题转化为无约束的问题，将原始问题转化为求极大极小问题的对偶问题，可以得到我们的最终结果：（略）

和第二部分中的结果唯一不同的是αi 的取值范围多了一个上限C 值，对于软间隔最大化时，其支持向量描述要复杂一些，因为其支持向量可以在间隔边界上（如下图中的虚线），也可以在间隔边界和超平面之间，或者在分离超平面误分的一侧。
![title](https://i.loli.net/2019/03/28/5c9c5a5a3537f.png)

## 4 合页损失函数
也称hinge损失函数，表达式：
![title](https://i.loli.net/2019/03/28/5c9c5ab7228b6.png)
因此我们上面的优化问题可以描述为：**（没有加软间隔松弛变量的惩罚）**
![title](https://i.loli.net/2019/03/28/5c9c5b26ea93f.png)
**对上述损失函数中的第一项可以理解为当样本分类正确且间隔大于1，即 yi(wxi + b) ≥ 1时，损失为0；而当 yi(wxi + b) < 1 时，损失为 1 - yi(wxi + b)，注意在这里即使样本分类正确，但是间隔小于1 的也会计入损失，这就是支持向量机的苛刻性。**
>分类正确且间隔大于1，则损失函数为0，分类错误或间隔小于1，损失函数都是1-yi(w*x+b)

## 5 线性不可分
上面讲到的软间隔最大化只能解决由于异常点而导致的线性不可分问题，**而对于本身的数据集就是非线性的问题就无能为力**，根据相关理论对于在低维空间线性不可分的问题，**一般将其映射到高维空间后都是线性可分的**，我们可以将这一理论运用到支持向量机中，可以引入一个函数 ϕ(x)，将样本集从当前维度映射到更高的维度，回过头来看我们之前的优化函数：
![title](https://i.loli.net/2019/03/28/5c9c5eaec0d5b.png)
**我们只需要将优化函数中的內积 xi  · xj 转化成 ϕ(xi) · ϕ(xj) 即可解决我们的非线性问题，**
內积的计算量也增大了，当映射后的维度很高，甚至是达到无穷维之后，求解模型时的计算量也会显著增大，那么如何来处理这个问题呢？这就需要引入我们的核函数了。
我们知道即使在映射到高维后，**內积 ϕ(xi) · ϕ(xj) 的值也依然是一个常数，那么是否存在这样一个函数 K( xi · xj ) = ϕ(xi) · ϕ(xj) ，我们将其称为核函数。**
>在此引出了支持向量机的第三个亮点：**不需要将样本映射到高维空间，而利用核函数解决非线性分类问题 。**

接下来我们介绍下常见的几种核函数：
1）线性核函数
   线性核函数很好理解，只能用来处理线性问题，其表达式如下：
$$k(x,y)=x^Ty+c$$
>因此我们可以将线性SVM和非线性SVM放在一起，只需要通过设定核函数和处理它们???(不懂）

2）多项式核函数
$$k(x,y)=(ax^T+c)^d$$
其中的 a，c，d 的值都是需要我们去通过调参设置的。

3）高斯核函数
径向基核函数也称为高斯核函数
$$k(x,y)=exp(-\frac{}{2\de})$$

　　

　　参数较少，只需要自己去设置参数 σ

　　4）Sigmoid 核函数

　　K(x, y) = tanh (ax · z + r)


