# 01 SVM原理及要点
**参考博客地址[微笑sun](https://www.cnblogs.com/jiangxinyang/p/9217424.html)**
>SVM原来处理二分类问题，经过进化，也可以处理多元线性分类和非线性的问题，同时处理回归问题。在深度学习兴起之前，是最好的分类算法。
**现在，在小样本的问题上，SVM仍然很好用**

## 1 感知机模型
>感知机模型是**二分类的线性分类器**，尝试找到超平面分开数据集，二维空间中，超平面是个直线，三维空间中则是一个平面。

感知机的模型：

$$f(x) = sign(W*x+b)$$

$sign(x)$是指示函数，自变量大于0取1，小于零取-1
![感知机](https://i.loli.net/2019/03/28/5c9c4c841e95f.png)

对于正确分类的点，$y(Wx+b) > 0$,对于错误分类的点，$<0$，所以，采用损失函数，使所有误分类的点到超平面的距离之和最小。

$$-\frac{1}{\begin{Vmatrix}w\end{Vmatrix}}\sum_{x_i\in{M}}y_i(w*x_i+b)$$

其中，M是表示**误分类的样本集合**，所以，最终得到的**感知机的损失函数**是：
$$L(w,b)=-\sum_{x_i\in{M}}y_i(w*x_i+b)$$

## 2 SVM
>感知机中，目标是将训练集分开，**而能够将样本分开的超平面有很多**。SVM本质上类似于感知机，但是要求却更加苛刻。

***远离超平面的点是安全的，容易被误分类的点离超平面很近。SVM的思想是重点关注离超平面很近的点***====>**++在分类正确的同时，让离超平面最近的点到超平面的距离最大++**
![title](https://i.loli.net/2019/03/28/5c9c52138cf6e.png)

**$\gamma$是离超平面最近的点的到超平面的几何间隔**，将几何间隔用函数间隔替代，可以将式子表示为：
![title](https://i.loli.net/2019/03/28/5c9c52a1c4577.png)
$\gamma$(^) 表示的是函数间隔，而函数间隔的取值是会随着w，b 成倍数变化而变化的，并不会影响最终的结果，因此令γ(帽子) = 1，则我们最终的问题可以表述为：
![title](https://i.loli.net/2019/03/28/5c9c5385cf832.png)
>这里得到了SVM的第一个**亮点：最大化间隔，最大化间隔能使得分类更加精确，且该最大间隔超平面是存在且唯一的。**
![title](https://i.loli.net/2019/03/28/5c9c53ff0a812.png)

根据凸优化理论，我们可以借助拉格朗日函数将我们的约束问题转化为无约束的问题来求解，我们的优化函数可以表达为：
![title](https://i.loli.net/2019/03/28/5c9c549d2d586.png)
其中，$\alpha_i$是拉格朗日乘子，都为正数。
然后，根据拉格朗日的对偶性，再转化为它的对偶问题的极大极小问题，（更容易求解）：
![title](https://i.loli.net/2019/03/28/5c9c5602d36ba.png)
先对w,b求导极小，得到w,b的值。然后，将得到的解带入拉格朗日函数，得到：
、、、、、
（省略）
、、、、、
转化成求$\alpha$的极小值问题，再引入KTT条件，得到：
yi(w*xi + b*) - 1 > 0 时，αi* = 0；当 αi* > 0 时，yi(w*xi + b*) - 1 = 0；
**得到SVM的第二个亮点：w，b 参数只与满足 yi(w*xi + b*) - 1 = 0 的样本有关，而这些样本点就是离最大间隔超平面最近的点，我们将这些点称之为*支持向量*。因此很多时候支持向量在小样本集分类时也能表现的很好，也正是因为这个原因。（另外需注意：α 向量的个数是和训练集数量相等的，对与大的训练集，会导致所需要的参数数量增多，因此SVM在处理大的训练集时会比其他常见的机器学习算法要慢）**

***因为参数只与支持向量有关，所以在小样本集上，效果很好。但是拉格朗日乘子的个数与训练集数量相等，所以SVM比照其他机器学习方法，略慢。***

## 3 软间隔最大化
>上面的硬间隔最大化无法处理线性不可分的问题，当存在异常点就会导致线性不可分。线性不可分以为之有些样本点的函数间隔不满足大于等于1的约束。因此对每个样本（xi,yi）引入一个松弛变量ξi， 则我们的约束条件变为：
![title](https://i.loli.net/2019/03/28/5c9c58ad6d377.png)

目标函数中加入对松弛变量的惩罚项，惩罚参数C > 0，目标优化函数变为：
![title](https://i.loli.net/2019/03/28/5c9c59b165bb2.png)




















