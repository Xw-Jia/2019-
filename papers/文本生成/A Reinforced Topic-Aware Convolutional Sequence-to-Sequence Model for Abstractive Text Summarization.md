# 一种用于抽象文本摘要的强化主题感知卷积序列到序列模型
## Abstract
在本文中，我们提出了一种深度学习方法，通过**将主题信息合并到卷积序列到序列（ConvS2S）模型并使用自我关键序列训练（SCST）进行优化来解决自动摘要任务**。
通过**共同关注主题和词级对齐**(jointly attending to topics and word-level alignment)，我们的方法可以通过有偏差的概率生成机制(biased probability generation mechanism)来提高生成的摘要的一致性，多样性和信息性。
另一方面，像SCST一样，强化训练直接优化了关于非可微度度量ROUGE(non-differentiable metric ROUGE)的所提出的模型，这也避免了推理期间的暴露偏差(exposure bias during inference)。
我们使用Gigaword，DUC-2004和LCSTS数据集上的最先进方法进行实验评估。
实证结果证明了我们提出的方法在抽象概括中的优越性.
## Introduction
**文本摘要的难点：**
>1. 自动文本摘要中的关键挑战是正确评估和选择重要信息，有效过滤冗余内容，以及正确聚合相关段并制作人类可读摘要
>2. 与输入和输出序列通常具有相似长度的机器转换任务不同，摘要任务更可能使输入和输出序列极不平衡。
>3. 此外，机器翻译任务通常在输入和输出序列之间具有一些直接的字级对齐，这在摘要中不太明显。 

**有两种类型的自动摘要技术，即提取和抽象。(extraction and abstraction)**
>1. 提取摘要的目标[Neto et al。，2002]是通过**选择源文档的重要部分并逐字连接来产生摘要**，
>2. 而抽象概括[Chopra et al。，2016]**基于核心思想生成摘要**。

>*抽象方法还应该能够正确地重写源文档的核心思想，并确保生成的摘要语法正确，便于人类阅读，这与人类进行摘要的方式非常接近*

**相关研究：**
>最近，深度神经网络模型已被广泛用于NLP任务，例如机器翻译[Bahdanau等，2014]和文本摘要[Nallapati等，2016b]。
特别是，**基于注意的序列 - 序列框架**[Bahdanau等，2014]与递归神经网络（RNNs）[Sutskever等，2014]在NLP任务中占优势。

**RNN的缺陷：**
>1. 然而，与基于CNN的模型([Dauphin等，2016])的分层结构相比，基于RNN的模型由于其非线性链结构而更容易出现梯度消失。
>2. 另外，RNN的**隐藏状态之间的时间依赖性阻止了序列元素的并行化**，这使得训练效率低下。

**本文的主要贡献：**
>在本文中，我们提出了一种**基于卷积序列到序列（ConvS2S）框架**[Gehring等，2017]与**主题感知注意机制相结合**的新方法。
据我们所知，这是自动抽象摘要的第一项工作，它**包含了主题信息，可以为深度学习架构提供主题和上下文对齐信息**。
此外，我们还通过**采用强化训练来优化**我们提出的模型
>1. 


